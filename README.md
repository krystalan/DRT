# Deep Reasoning Translation (DRT) Project

<p align="center">
ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-7B">DRT-7B</a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-8B">DRT-8B</a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-14B">DRT-14B</a>&nbsp&nbsp | &nbsp&nbsp ğŸ¤— <a href="https://huggingface.co/Krystalan/DeepTrans-7B">DeepTrans-7B</a> | &nbsp&nbsp ğŸ¤— <a href="https://huggingface.co/Krystalan/ExTrans-7B">ExTrans-7B</a> | &nbsp&nbsp ğŸ¤— <a href="https://huggingface.co/Krystalan/mExTrans-7B">mExTrans-7B</a>

</p>

This repository contains the resources for our work:
- [**DeepTrans**: Deep Reasoning Translation via Reinforcement Learning](https://arxiv.org/abs/2504.10187) (**TACL 2025**)  
- [**DRT**: Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/abs/2412.17498) (**ACL 2025 Findings**)  
- [**ExTrans**: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996) (arXiv preprint 2025)



Our exploration route is as follows:

![](./images/exploration_route.png)

### Updates:
- *2025.08.21*: Our [DeepTrans paper](https://arxiv.org/abs/2504.10187) is accepted to **TACL**.
- *2025.05.27*: We released the full data of our DRT work, including the synthesized thought contents and translations. Please refer to `data/MetaphorTrans_*.jsonl`
- *2025.05.19*: We released [ExTrans paper](https://arxiv.org/abs/2505.12996) with ğŸ¤— <a href="https://huggingface.co/Krystalan/ExTrans-7B">ExTrans-7B</a> and ğŸ¤— <a href="https://huggingface.co/Krystalan/mExTrans-7B">mExTrans-7B</a>. Check it out!
- *2025.05.16*: Our [DRT paper](https://arxiv.org/abs/2412.17498) is accepted to **ACL 2025 Findings**.
- *2025.04.14*: We released [DeepTrans paper](https://arxiv.org/abs/2504.10187) with ğŸ¤— <a href="https://huggingface.co/Krystalan/DeepTrans-7B">DeepTrans-7B</a>. Check it out!
- *2024.12.31*: We updated [DRT paper](https://arxiv.org/abs/2412.17498) with more detals and analyses. Check it out!
- *2024.12.24*: We released [DRT paper](https://arxiv.org/abs/2412.17498) with ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-7B">DRT-7B</a>, ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-8B">DRT-8B</a> and ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-14B">DRT-14B</a>. Check it out!


If you find this work is useful, please consider cite our paper:
```
@article{wang2025deeptrans,
  title={Deep Reasoning Translation via Reinforcement Learning},
  author={Wang, Jiaan and Meng, Fandong and Zhou, Jie},
  journal={arXiv preprint arXiv:2504.10187},
  year={2025}
}
```

```
@inproceedings{wang-etal-2025-drt,
    title = "{DRT}: Deep Reasoning Translation via Long Chain-of-Thought",
    author = "Wang, Jiaan  and
      Meng, Fandong  and
      Liang, Yunlong  and
      Zhou, Jie",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.351/",
    doi = "10.18653/v1/2025.findings-acl.351",
    pages = "6770--6782",
    ISBN = "979-8-89176-256-5"
}
```

```
@article{wang2025extrans,
  title={ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning},
  author={Wang, Jiaan and Meng, Fandong and Zhou, Jie},
  journal={arXiv preprint arXiv:2505.12996},
  year={2025}
}
```

# Quick Links
- [DRT: Deep Reasoning Translation via Long Chain-of-Thought (ACL 2025 Findings)](#drt)
    - [Introduction](#introduction)
    - [Models](#models)
        - [Model Access](#model-access)
        - [Model Performance](#model-performance)
        - [Model Prompts](#model-prompts)
        - [Quickstart](#quickstart)
    - [Translation Cases](#translation-cases)
    - [Data (MetaphorTrans)](#data)
- [DeepTrans: Deep Reasoning Translation via Reinforcement Learning (TACL)](#deeptrans)
    - [Model Checkpoint](#model-checkpoint)
    - [Inference](#inference)
    - [Data](#book-data)
- [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](#extrans)
    - [ExTrans-7B inference](#inference-of-extrans)
    - [mExTrans-7B inference](#inference-of-mextrans)
- [License](#license)

# DRT

## Introduction

![](./images/multi-agent-framework.png)



In this work, we introduce DRT, an attempt to bring the success of long thought reasoning to neural machine translation (MT). To this end,
- ğŸŒŸ We mine English sentences with similes or metaphors from existing literature books, which are suitable for translation via long thought.
- ğŸŒŸ We propose a designed multi-agent framework with three agents (i.e., a translator, an advisor and an evaluator) to synthesize the MT samples with long thought. There are 22,264 synthesized samples in total.
- ğŸŒŸ We train DRT-8B, DRT-7B and DRT-14B using Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct as backbones.

> Our goal is not to achieve competitive performance with OpenAIâ€™s O1 in neural machine translation (MT). Instead, we explore technical routes to bring the success of long thought to MT. To this end, we introduce DRT, *a byproduct of our exploration*, and we hope it could facilitate the corresponding research in this direction.

![](./images/data_case.png)


## Models

### Model Access

|  | Backbone | Model Access |
| :--: | :--: | :--: |
| DRT-7B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-7B">DRT-7B</a> |
| DRT-8B | ğŸ¤— <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.1-8B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-8B">DRT-8B</a> |
| DRT-14B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-14B-Instruct">Qwen2.5-14B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-14B">DRT-14B</a> |

### Model Performance
|  | GRF | CometKiwi | GRB | BLEU | CometScore |
| :--: | :--: | :--: | :--: | :--: | :--: |
| Llama-3.1-8B-Instruct | 79.25 | 70.14 | 73.30 | 18.55 | 74.58 |
| Qwen2.5-7B-Instruct | 81.53 | 70.36 | 77.92 | 27.02 | 76.78 |
| Qwen2.5-14B-Instruct | 84.74 | 72.01 | 80.85 | 30.23 | 78.84 |
| Marco-o1-7B | 82.41 | 71.62 | 77.50 | 29.48 | 77.41 |
| QwQ-32B-preview | 86.31 | 71.48 | 83.08 | 27.46 | 78.68 |
| DRT-8B | 84.49 | 70.85 | 80.80 | 32.67 | 78.81 |
| DRT-7B | 85.57 | 71.78 | 82.38 | 35.54 | 80.19 |
| DRT-14B | **87.19** | **72.11** | **83.20** | **36.46** | **80.64** |


### Model Prompts
During model inference, please use the following prompts:
- System prompt: `You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight.`
- User prompt: `Please translate the following text from English to Chinese:\n[An English text]`

DRT models will first generate the thought and then provide the final translation, with the following format:
```
<thought>
[Reasoning process]
</thought>
<output>
[Final translation]
</output>
```

### Quickstart
- â›·ï¸ Huggingface Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Krystalan/DRT-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Please translate the following text from English to Chinese:\nThe mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."
messages = [
    {"role": "system", "content": "You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

- â›·ï¸ vllm:

Deploying LLMs:
```bash
python3 -m vllm.entrypoints.openai.api_server --model [model_ckpt] --served-model-name [model_name]
```

Calling LLMs:
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model=[model_name],
    messages=[
        {"role": "system", "content": "You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight."},
        {"role": "user", "content": "Please translate the following text from English to Chinese:\nThe mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."},
    ],
    temperature=0.1,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```

## Translation Cases

| Source Sentence | Qwen2.5-14B-Instruct | QwQ-32B-preview | DRT-14B |
| :--: | :--: | :--: | :--: |
|Women were weeping and children crying, and all were going as fast as seemingly lay in their power, looking behind now and then as if pursued by some deadly enemy. | å¦‡å¥³åœ¨å“­æ³£ï¼Œå­©å­ä»¬åœ¨å“€å·ï¼Œæ‰€æœ‰äººéƒ½ä¼¼ä¹åœ¨å°½å…¨åŠ›å¿«é€Ÿå‰è¡Œï¼Œä¸æ—¶å›å¤´å¼ æœ›ï¼Œä»¿ä½›èƒŒåæœ‰æŸä¸ªè‡´å‘½çš„æ•Œäººåœ¨è¿½èµ¶ã€‚ | å¦‡å¥³ä»¬å“­æ³£ç€ï¼Œå­©å­ä»¬å“­å–Šç€ï¼Œæ¯ä¸ªäººéƒ½åœ¨å°½åŠ›åŠ å¿«è„šæ­¥ï¼Œä¸æ—¶å›å¤´å¼ æœ›ï¼Œä»¿ä½›èƒŒåæœ‰è‡´å‘½çš„æ•Œäººåœ¨è¿½å‡»ã€‚ | å¦‡å¥³æ‚²æ³£ï¼Œå­©ç«¥å“€å·ï¼Œä¼—äººç–¾å¥”ï¼Œä¸æ—¶å›é¦–ï¼Œä»¿ä½›æœ‰è‡´å‘½ä¹‹æ•Œåœ¨åç´§è¿½ã€‚ |
|Wait through this season of the rain Till suns of autumn dry the plain, Then shall thy giant foe, and all His host and realm, before thee fall. | ç­‰å¾…è¿™é›¨å­£è¿‡å»ï¼Œç›´åˆ°ç§‹æ—¥çš„é˜³å…‰æ™’å¹²åŸé‡ï¼Œé‚£æ—¶ä½ çš„å·¨æ•ŒåŠå…¶éƒ¨ä¼—å’Œç‹å›½éƒ½å°†å€’åœ¨ä½ é¢å‰ã€‚ | å¿è¿‡é›¨å­£æ¼«é•¿å¤œï¼Œå¾…ç§‹é˜³å‡ç©ºï¼Œæ™’å¹²å¤§åœ°æ¹¿æ¶¦è‰²ï¼Œé‚£æ—¶ï¼Œå·¨æ•Œå®å°ï¼Œä»–çš„å†›é˜Ÿå’Œç‹å›½ï¼Œå…¨åœ¨ä½ è„šä¸‹ã€‚ | å†å°½è¿™é›¨å­£ï¼Œå¾…ç§‹æ—¥é˜³å…‰é©±æ•£å¹³åŸä¸Šçš„æœ€åä¸€ä¸æ¹¿æ¶¦ï¼Œé‚£æ—¶ï¼Œä½ é‚£å·¨æ•Œï¼Œè¿åŒä»–çš„å†›é˜Ÿä¸ç‹å›½ï¼Œéƒ½å°†åŒåŒåœ¨ä½ è„šä¸‹ã€‚ |
|And if their words had taken another turn...if he himself had been less fastidious about intruding on another man's secrets...it was cruel to think how thin a film had shut out rescue from all this guilt and misery. | è€Œå¦‚æœä»–ä»¬çš„è¨€è¾è½¬å‘äº†å¦ä¸€ä¸ªæ–¹å‘â€¦â€¦å¦‚æœä»–è‡ªå·±å¯¹ä¾µæ‰°ä»–äººç§˜å¯†ä¸é‚£ä¹ˆä»‹æ„â€¦â€¦æƒ³åˆ°æ­£æ˜¯é‚£å±‚è–„è–„çš„éš”é˜‚å°†æ•‘æ´æ‹’ä¹‹é—¨å¤–ï¼Œä½¿å¾—è¿™ä¸€åˆ‡çš„ç½ªæ¶å’Œç—›è‹¦æ— æ³•é¿å…ï¼ŒçœŸæ˜¯æ®‹é…·ã€‚ | è€Œä¸”ï¼Œå‡å¦‚ä»–ä»¬çš„è°ˆè¯èµ°å‘ä¸åŒâ€¦â€¦å‡å¦‚ä»–è‡ªå·±ä¸å¯¹å¹²æ¶‰ä»–äººç§äº‹å¦‚æ­¤è°¨æ…â€¦â€¦é‚£æ—¶ä»–ä¼šæ„è¯†åˆ°ï¼Œåªè¦ä¸€å±‚è–„è–„çš„å±éšœå°±é˜»éš”äº†æ•‘æ´ï¼Œä½¿å¾—æ‰€æœ‰è¿™äº›ç½ªæ¶å’Œç—›è‹¦æ— æ³•è§£è„±ï¼Œè¿™çœŸæ˜¯æ®‹å¿ã€‚ | è‹¥ä»–ä»¬çš„è¨€è¾ç¨ä½œè½¬å˜â€¦â€¦è‹¥ä»–æœ¬äººä¸é‚£ä¹ˆä»‹æ„ä¾µå…¥ä»–äººçš„å¿ƒæ‰‰â€¦â€¦æƒ³åˆ°è¿™å±‚è–„è–„çš„éš”é˜‚ç«Ÿå°†æ‰€æœ‰å†…ç–šä¸ç—›è‹¦éš”ç¦»äºæ•‘æ´ä¹‹å¤–ï¼Œå®ä¸ºæ®‹å¿ã€‚ |
|This cold officer upon a monument, who dropped epithets unconcernedly down, would be finer as a dead man, he thought. | ä»–è®¤ä¸ºï¼Œè¿™ä¸ªç«™åœ¨çºªå¿µç¢‘ä¸Šçš„å†·æ¼ å®˜å‘˜ï¼Œè‹¥æ­»äº†ä¼šæ›´å¥½ï¼Œä»–ä¸å¸¦ä»»ä½•æ„Ÿæƒ…åœ°æŠ›ä¸‹äº†ä¸€äº›ç§°å‘¼ã€‚ | è¿™ä¸ªå†·å†°å†°çš„å®˜å‘˜ç«™åœ¨çºªå¿µç¢‘ä¸Šï¼Œæ¯«ä¸åœ¨æ„åœ°æŠ›ä¸‹ä¸€äº›ç§°å·ï¼Œä»–æƒ³ï¼Œå¦‚æœä½œä¸ºä¸€ä¸ªæ­»äººä¼šæ›´å‡ºè‰²ã€‚ | çºªå¿µç¢‘ä¸Šçš„å†·æ·¡å®˜å‘˜ï¼Œæ¼«ä¸ç»å¿ƒåœ°åŸå’é‚£äº›ä¿®é¥°è¯­ï¼Œä»–å¿ƒæƒ³ï¼Œè‹¥åŒ–ä¸ºäº¡è€…ï¼Œæˆ–è®¸æ›´æ˜¾å°Šè´µã€‚ |


## Data

We release the synthesized data (named ```MetaphorTrans```), please refer to `data/MetaphorTrans_*.jsonl`, where `text` and `trans` denote the source English sentences and the target Chinese translations, respectively. `thought` indicates the thought content for MT.


# DeepTrans

![](./images/deeptrans-reward-framework.png)

In this work, we propose DeepTrans-7B, which aims at enhancing the free translation ability of deep reasoning LLMs via RL. To this end, we use DeepSeek-v3 (671B) as the reward model, and design scoring criteria on both translations and thought process.

## Model Checkpoint

|  | Backbone | Model Access |
| :--: | :--: | :--: |
| DeepTrans-7B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DeepTrans-7B">DeepTrans-7B</a> |

## Inference

- Huggingface Transformers
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Krystalan/DeepTrans-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚ä½ åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­éå¸¸æ“…é•¿æ€è€ƒï¼Œä¼šå…ˆè¿›è¡Œæ€è€ƒå†ç»™å‡ºç¿»è¯‘ç»“æœã€‚ä½ çš„è¾“å‡ºæ ¼å¼ä¸ºï¼š\n<think>\n[æ€è€ƒè¿‡ç¨‹]\n</think>[ç¿»è¯‘ç»“æœ]\n\nåœ¨ä½ æ€è€ƒå®Œä¹‹åï¼Œä¹Ÿå°±æ˜¯</think>ä¹‹åï¼Œä½ ä¼šç»™å‡ºæœ€ç»ˆçš„ç¿»è¯‘å³â€œ[ç¿»è¯‘ç»“æœ]â€ï¼Œä¸”[ç¿»è¯‘ç»“æœ]ä¸­ä¸éœ€è¦ç»™å‡ºä»»ä½•è§£é‡Šå’Œæè¿°ï¼Œåªéœ€è¦æä¾›è‹±æ–‡çš„ç¿»è¯‘ç»“æœã€‚\nç°åœ¨è¯·ä½ ç¿»è¯‘ä»¥ä¸‹è¿™å¥è‹±è¯­ï¼š\n" + "The mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."

messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

- VLLM:
deploying LLMs:
```bash
python3 -m vllm.entrypoints.openai.api_server --model [model_ckpt] --served-model-name [model_name]
```

calling LLMs:
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚ä½ åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­éå¸¸æ“…é•¿æ€è€ƒï¼Œä¼šå…ˆè¿›è¡Œæ€è€ƒå†ç»™å‡ºç¿»è¯‘ç»“æœã€‚ä½ çš„è¾“å‡ºæ ¼å¼ä¸ºï¼š\n<think>\n[æ€è€ƒè¿‡ç¨‹]\n</think>[ç¿»è¯‘ç»“æœ]\n\nåœ¨ä½ æ€è€ƒå®Œä¹‹åï¼Œä¹Ÿå°±æ˜¯</think>ä¹‹åï¼Œä½ ä¼šç»™å‡ºæœ€ç»ˆçš„ç¿»è¯‘å³â€œ[ç¿»è¯‘ç»“æœ]â€ï¼Œä¸”[ç¿»è¯‘ç»“æœ]ä¸­ä¸éœ€è¦ç»™å‡ºä»»ä½•è§£é‡Šå’Œæè¿°ï¼Œåªéœ€è¦æä¾›è‹±æ–‡çš„ç¿»è¯‘ç»“æœã€‚\nç°åœ¨è¯·ä½ ç¿»è¯‘ä»¥ä¸‹è¿™å¥è‹±è¯­ï¼š\n" + "The mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."

chat_response = client.chat.completions.create(
    model=[model_name],
    messages=[
        {"role": "user", "content": prompt},
    ],
    temperature=0.1,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```

## Book Data

We purchase electronic copies of two complete literature books, i.e., *The Essential O. Henry Collection* and *Orbital*, and evaluate model performance on these two books.

We released the content of *The Essential O. Henry Collection* at `data/ohenry.txt`. For the content of *Orbital*, we cannot release it due to copyright protection. If you indeed need the data **only for research purposes**, please send an application email to jawang.nlp[at]gmail.com to obtain it.

# ExTrans

![](./images/extrans-reward-framework.png)

In this work, we propose ExTrans-7B, which aims at enhancing the free translation ability of deep reasoning LLMs via **exemplar-enhanced** RL. In detail, for each training MT sample, we use DeepSeek-R1 (671B) to generate a exemplar translation, and compare the translation results of the policy model with the exemplar translations to provide rewards for the policy model.

Moreover, we extend ExTrans-7B from English-to-Chinese translation into **multilingual settings** with 11 languages, *e.g.*, Chinese, English, Arabic, Czech, German, Spanish, French, Italian, Japanese, Russian and Korean. 

The model checkpoints can be accessed from the following links:  
|  | Backbone | Model Access |
| :--: | :--: | :--: |
| ExTrans-7B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/ExTrans-7B">ExTrans-7B</a> |
| mExTrans-7B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/mExTrans-7B">mExTrans-7B</a> |


## Inference of ExTrans

deploying LLMs:
```bash
python3 -m vllm.entrypoints.openai.api_server --model [model_ckpt] --served-model-name [model_name]
```

calling LLMs:
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚ä½ åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­éå¸¸æ“…é•¿æ€è€ƒï¼Œä¼šå…ˆè¿›è¡Œæ€è€ƒå†ç»™å‡ºç¿»è¯‘ç»“æœã€‚ä½ çš„è¾“å‡ºæ ¼å¼ä¸ºï¼š\n<think>\n[æ€è€ƒè¿‡ç¨‹]\n</think>[ç¿»è¯‘ç»“æœ]\n\nåœ¨ä½ æ€è€ƒå®Œä¹‹åï¼Œä¹Ÿå°±æ˜¯</think>ä¹‹åï¼Œä½ ä¼šç»™å‡ºæœ€ç»ˆçš„ç¿»è¯‘å³â€œ[ç¿»è¯‘ç»“æœ]â€ï¼Œä¸”[ç¿»è¯‘ç»“æœ]ä¸­ä¸éœ€è¦ç»™å‡ºä»»ä½•è§£é‡Šå’Œæè¿°ï¼Œåªéœ€è¦æä¾›è‹±æ–‡çš„ç¿»è¯‘ç»“æœã€‚\nç°åœ¨è¯·ä½ ç¿»è¯‘ä»¥ä¸‹è¿™å¥è‹±è¯­ï¼š\n" + "The mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."

chat_response = client.chat.completions.create(
    model=[model_name],
    messages=[
        {"role": "user", "content": prompt},
    ],
    temperature=0.1,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```

## Inference of mExTrans

calling LLMs:
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

lang2des = {
    "ar": "é˜¿æ‹‰ä¼¯è¯­", # Arabic
    "cs": "æ·å…‹è¯­", # Czech
    "de": "å¾·è¯­", # German
    "en": "è‹±è¯­", # English
    "es": "è¥¿ç­ç‰™è¯­", # Spanish
    "fr": "æ³•è¯­", # French
    "it": "æ„å¤§åˆ©è¯­", # Italian
    "ja": "æ—¥è¯­", # Japanese
    "ko": "éŸ©è¯­", # Korean
    "ru": "ä¿„è¯­", # Russian
    "zh": "ä¸­æ–‡" # Chinese
}

current_src_lang = lang2des["en"] # set the source language
current_trg_lang = lang2des["zh"] # set the target language

current_sent = "The mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap." # the source sentence to be translated

TRANS_PROMPT = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†{current_src}ç¿»è¯‘æˆ{current_trg}ã€‚ä½ åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­éå¸¸æ“…é•¿æ€è€ƒï¼Œä¼šå…ˆç”¨ä¸­æ–‡è¿›è¡Œæ€è€ƒå†ç»™å‡ºç¿»è¯‘ç»“æœã€‚åœ¨ä½ æ€è€ƒå®Œä¹‹åï¼Œä¹Ÿå°±æ˜¯</think>ä¹‹åï¼Œä½ ä¼šç»™å‡ºæœ€ç»ˆçš„ç¿»è¯‘ï¼Œä¸”æœ€ç»ˆçš„ç¿»è¯‘ç»“æœä¸­ä¸éœ€è¦ç»™å‡ºä»»ä½•è§£é‡Šå’Œæè¿°ï¼Œåªéœ€è¦æä¾›ç¿»è¯‘ç»“æœã€‚\nç°åœ¨è¯·ä½ ç¿»è¯‘ä»¥ä¸‹è¿™å¥{current_src}ï¼š\n{current_sent}"

chat_response = client.chat.completions.create(
    model=[model_name],
    messages=[
        {"role": "user", "content": TRANS_PROMPT.format(current_src=current_src_lang, current_trg=current_trg_lang, current_sent=current_sent)},
    ],
    temperature=0.1,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```

Note that, the prompt of mExTrans is slightly different from that of ExTrans.

# License
This work is licensed under cc-by-nc-sa-4.0

