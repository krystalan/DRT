# DRT & DeepTrans

<p align="center">
ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-7B">DRT-7B</a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-8B">DRT-8B</a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-14B">DRT-14B</a>&nbsp&nbsp | &nbsp&nbsp ğŸ¤— <a href="https://huggingface.co/Krystalan/DeepTrans-7B">DeepTrans-7B</a>

</p>

This repository contains the resources for our work:
- [Deep Reasoning Translation via Reinforcement Learning](https://arxiv.org/abs/2504.10187)
- [DRT: Deep Reasoning Translation via Long Chain-of-Thought](https://arxiv.org/abs/2412.17498)



<!-- ### Updates:
- *2024.12.31*: We updated [our paper](https://arxiv.org/abs/2412.17498) with more detals and analyses. Check it out!
- *2024.12.31*: We released the testing set of our work, please refer to `data/test.jsonl`
- *2024.12.30*: We released a new model checkpoint using Llama-3.1-8B-Instruct as the backbone, i.e., ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-o1-8B">DRT-o1-8B</a>
- *2024.12.24*: We released [our paper](https://arxiv.org/abs/2412.17498). Check it out!
- *2024.12.23*: We released our model checkpoints. ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-o1-7B">DRT-o1-7B</a> and ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-o1-14B">DRT-o1-14B</a>. -->


If you find this work is useful, please consider cite our paper:
```
@article{wang2025deeptrans,
  title={Deep Reasoning Translation via Reinforcement Learning},
  author={Wang, Jiaan and Meng, Fandong and Zhou, Jie},
  journal={arXiv preprint arXiv:2504.10187},
  year={2025}
}
```

```
@article{wang2024drt,
  title={DRT: Deep Reasoning Translation via Long Chain-of-Thought},
  author={Wang, Jiaan and Meng, Fandong and Liang, Yunlong and Zhou, Jie},
  journal={arXiv preprint arXiv:2412.17498},
  year={2024}
}
```

# Quick Links
- [DRT](#drt)
    - [Introduction](#introduction)
    - [Models](#models)
        - [Model Access](#model-access)
        - [Model Performance](#model-performance)
        - [Model Prompts](#model-prompts)
        - [Quickstart](#quickstart)
    - [Translation Cases](#translation-cases)
    - [Data](#data)
- [DeepTrans](#deeptrans)
    - [Model Checkpoint](#model-checkpoint)
    - [Inference](#inference)
- [License](#license)

# DRT

## Introduction

![](./images/multi-agent-framework.png)



In this work, we introduce DRT, an attempt to bring the success of long thought reasoning to neural machine translation (MT). To this end,
- ğŸŒŸ We mine English sentences with similes or metaphors from existing literature books, which are suitable for translation via long thought.
- ğŸŒŸ We propose a designed multi-agent framework with three agents (i.e., a translator, an advisor and an evaluator) to synthesize the MT samples with long thought. There are 22,264 synthesized samples in total.
- ğŸŒŸ We train DRT-8B, DRT-7B and DRT-14B using Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct as backbones.

> Our goal is not to achieve competitive performance with OpenAIâ€™s O1 in neural machine translation (MT). Instead, we explore technical routes to bring the success of long thought to MT. To this end, we introduce DRT, *a byproduct of our exploration*, and we hope it could facilitate the corresponding research in this direction.

![](./images/data_case.png)


## Models

### Model Access

|  | Backbone | Model Access |
| :--: | :--: | :--: |
| DRT-7B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-7B">DRT-7B</a> |
| DRT-8B | ğŸ¤— <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.1-8B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-8B">DRT-8B</a> |
| DRT-14B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-14B-Instruct">Qwen2.5-14B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DRT-14B">DRT-14B</a> |

### Model Performance
|  | GRF | CometKiwi | GRB | BLEU | CometScore |
| :--: | :--: | :--: | :--: | :--: | :--: |
| Llama-3.1-8B-Instruct | 79.25 | 70.14 | 73.30 | 18.55 | 74.58 |
| Qwen2.5-7B-Instruct | 81.53 | 70.36 | 77.92 | 27.02 | 76.78 |
| Qwen2.5-14B-Instruct | 84.74 | 72.01 | 80.85 | 30.23 | 78.84 |
| Marco-o1-7B | 82.41 | 71.62 | 77.50 | 29.48 | 77.41 |
| QwQ-32B-preview | 86.31 | 71.48 | 83.08 | 27.46 | 78.68 |
| DRT-8B | 84.49 | 70.85 | 80.80 | 32.67 | 78.81 |
| DRT-7B | 85.57 | 71.78 | 82.38 | 35.54 | 80.19 |
| DRT-14B | **87.19** | **72.11** | **83.20** | **36.46** | **80.64** |


### Model Prompts
During model inference, please use the following prompts:
- System prompt: `You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight.`
- User prompt: `Please translate the following text from English to Chinese:\n[An English text]`

DRT models will first generate the thought and then provide the final translation, with the following format:
```
<thought>
[Reasoning process]
</thought>
<output>
[Final translation]
</output>
```

### Quickstart
- â›·ï¸ Huggingface Transformers:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Krystalan/DRT-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Please translate the following text from English to Chinese:\nThe mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."
messages = [
    {"role": "system", "content": "You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

- â›·ï¸ vllm:

Deploying LLMs:
```bash
python3 -m vllm.entrypoints.openai.api_server --model [model_ckpt] --served-model-name [model_name]
```

Calling LLMs:
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model=[model_name],
    messages=[
        {"role": "system", "content": "You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight."},
        {"role": "user", "content": "Please translate the following text from English to Chinese:\nThe mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."},
    ],
    temperature=0.1,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```

## Translation Cases

| Source Sentence | Qwen2.5-14B-Instruct | QwQ-32B-preview | DRT-14B |
| :--: | :--: | :--: | :--: |
|Women were weeping and children crying, and all were going as fast as seemingly lay in their power, looking behind now and then as if pursued by some deadly enemy. | å¦‡å¥³åœ¨å“­æ³£ï¼Œå­©å­ä»¬åœ¨å“€å·ï¼Œæ‰€æœ‰äººéƒ½ä¼¼ä¹åœ¨å°½å…¨åŠ›å¿«é€Ÿå‰è¡Œï¼Œä¸æ—¶å›å¤´å¼ æœ›ï¼Œä»¿ä½›èƒŒåæœ‰æŸä¸ªè‡´å‘½çš„æ•Œäººåœ¨è¿½èµ¶ã€‚ | å¦‡å¥³ä»¬å“­æ³£ç€ï¼Œå­©å­ä»¬å“­å–Šç€ï¼Œæ¯ä¸ªäººéƒ½åœ¨å°½åŠ›åŠ å¿«è„šæ­¥ï¼Œä¸æ—¶å›å¤´å¼ æœ›ï¼Œä»¿ä½›èƒŒåæœ‰è‡´å‘½çš„æ•Œäººåœ¨è¿½å‡»ã€‚ | å¦‡å¥³æ‚²æ³£ï¼Œå­©ç«¥å“€å·ï¼Œä¼—äººç–¾å¥”ï¼Œä¸æ—¶å›é¦–ï¼Œä»¿ä½›æœ‰è‡´å‘½ä¹‹æ•Œåœ¨åç´§è¿½ã€‚ |
|Wait through this season of the rain Till suns of autumn dry the plain, Then shall thy giant foe, and all His host and realm, before thee fall. | ç­‰å¾…è¿™é›¨å­£è¿‡å»ï¼Œç›´åˆ°ç§‹æ—¥çš„é˜³å…‰æ™’å¹²åŸé‡ï¼Œé‚£æ—¶ä½ çš„å·¨æ•ŒåŠå…¶éƒ¨ä¼—å’Œç‹å›½éƒ½å°†å€’åœ¨ä½ é¢å‰ã€‚ | å¿è¿‡é›¨å­£æ¼«é•¿å¤œï¼Œå¾…ç§‹é˜³å‡ç©ºï¼Œæ™’å¹²å¤§åœ°æ¹¿æ¶¦è‰²ï¼Œé‚£æ—¶ï¼Œå·¨æ•Œå®å°ï¼Œä»–çš„å†›é˜Ÿå’Œç‹å›½ï¼Œå…¨åœ¨ä½ è„šä¸‹ã€‚ | å†å°½è¿™é›¨å­£ï¼Œå¾…ç§‹æ—¥é˜³å…‰é©±æ•£å¹³åŸä¸Šçš„æœ€åä¸€ä¸æ¹¿æ¶¦ï¼Œé‚£æ—¶ï¼Œä½ é‚£å·¨æ•Œï¼Œè¿åŒä»–çš„å†›é˜Ÿä¸ç‹å›½ï¼Œéƒ½å°†åŒåŒåœ¨ä½ è„šä¸‹ã€‚ |
|And if their words had taken another turn...if he himself had been less fastidious about intruding on another man's secrets...it was cruel to think how thin a film had shut out rescue from all this guilt and misery. | è€Œå¦‚æœä»–ä»¬çš„è¨€è¾è½¬å‘äº†å¦ä¸€ä¸ªæ–¹å‘â€¦â€¦å¦‚æœä»–è‡ªå·±å¯¹ä¾µæ‰°ä»–äººç§˜å¯†ä¸é‚£ä¹ˆä»‹æ„â€¦â€¦æƒ³åˆ°æ­£æ˜¯é‚£å±‚è–„è–„çš„éš”é˜‚å°†æ•‘æ´æ‹’ä¹‹é—¨å¤–ï¼Œä½¿å¾—è¿™ä¸€åˆ‡çš„ç½ªæ¶å’Œç—›è‹¦æ— æ³•é¿å…ï¼ŒçœŸæ˜¯æ®‹é…·ã€‚ | è€Œä¸”ï¼Œå‡å¦‚ä»–ä»¬çš„è°ˆè¯èµ°å‘ä¸åŒâ€¦â€¦å‡å¦‚ä»–è‡ªå·±ä¸å¯¹å¹²æ¶‰ä»–äººç§äº‹å¦‚æ­¤è°¨æ…â€¦â€¦é‚£æ—¶ä»–ä¼šæ„è¯†åˆ°ï¼Œåªè¦ä¸€å±‚è–„è–„çš„å±éšœå°±é˜»éš”äº†æ•‘æ´ï¼Œä½¿å¾—æ‰€æœ‰è¿™äº›ç½ªæ¶å’Œç—›è‹¦æ— æ³•è§£è„±ï¼Œè¿™çœŸæ˜¯æ®‹å¿ã€‚ | è‹¥ä»–ä»¬çš„è¨€è¾ç¨ä½œè½¬å˜â€¦â€¦è‹¥ä»–æœ¬äººä¸é‚£ä¹ˆä»‹æ„ä¾µå…¥ä»–äººçš„å¿ƒæ‰‰â€¦â€¦æƒ³åˆ°è¿™å±‚è–„è–„çš„éš”é˜‚ç«Ÿå°†æ‰€æœ‰å†…ç–šä¸ç—›è‹¦éš”ç¦»äºæ•‘æ´ä¹‹å¤–ï¼Œå®ä¸ºæ®‹å¿ã€‚ |
|This cold officer upon a monument, who dropped epithets unconcernedly down, would be finer as a dead man, he thought. | ä»–è®¤ä¸ºï¼Œè¿™ä¸ªç«™åœ¨çºªå¿µç¢‘ä¸Šçš„å†·æ¼ å®˜å‘˜ï¼Œè‹¥æ­»äº†ä¼šæ›´å¥½ï¼Œä»–ä¸å¸¦ä»»ä½•æ„Ÿæƒ…åœ°æŠ›ä¸‹äº†ä¸€äº›ç§°å‘¼ã€‚ | è¿™ä¸ªå†·å†°å†°çš„å®˜å‘˜ç«™åœ¨çºªå¿µç¢‘ä¸Šï¼Œæ¯«ä¸åœ¨æ„åœ°æŠ›ä¸‹ä¸€äº›ç§°å·ï¼Œä»–æƒ³ï¼Œå¦‚æœä½œä¸ºä¸€ä¸ªæ­»äººä¼šæ›´å‡ºè‰²ã€‚ | çºªå¿µç¢‘ä¸Šçš„å†·æ·¡å®˜å‘˜ï¼Œæ¼«ä¸ç»å¿ƒåœ°åŸå’é‚£äº›ä¿®é¥°è¯­ï¼Œä»–å¿ƒæƒ³ï¼Œè‹¥åŒ–ä¸ºäº¡è€…ï¼Œæˆ–è®¸æ›´æ˜¾å°Šè´µã€‚ |


## Data

We release the testing set of our work, please refer to `data/test.jsonl`, where `en` indicates the English source sentences, and `zh` denotes the corresponding Chinese translation.

We will release the long-thought MT data as well as the data collection codes soon!


# DeepTrans

![](./images/deeptrans-reward-framework.png)

In this work, we propose DeepTrans-7B, which aims at enhancing the free translation ability of deep reasoning LLMs via RL. To this end, we use DeepSeek-v3 (671B) as the reward model, and design scoring criteria on both translations and thought process.

## Model Checkpoint

|  | Backbone | Model Access |
| :--: | :--: | :--: |
| DeepTrans-7B | ğŸ¤— <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a> | ğŸ¤— <a href="https://huggingface.co/Krystalan/DeepTrans-7B">DeepTrans-7B</a> |

## Inference

- Huggingface Transformers
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Krystalan/DeepTrans-7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚ä½ åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­éå¸¸æ“…é•¿æ€è€ƒï¼Œä¼šå…ˆè¿›è¡Œæ€è€ƒå†ç»™å‡ºç¿»è¯‘ç»“æœã€‚ä½ çš„è¾“å‡ºæ ¼å¼ä¸ºï¼š\n<think>\n[æ€è€ƒè¿‡ç¨‹]\n</think>[ç¿»è¯‘ç»“æœ]\n\nåœ¨ä½ æ€è€ƒå®Œä¹‹åï¼Œä¹Ÿå°±æ˜¯</think>ä¹‹åï¼Œä½ ä¼šç»™å‡ºæœ€ç»ˆçš„ç¿»è¯‘å³â€œ[ç¿»è¯‘ç»“æœ]â€ï¼Œä¸”[ç¿»è¯‘ç»“æœ]ä¸­ä¸éœ€è¦ç»™å‡ºä»»ä½•è§£é‡Šå’Œæè¿°ï¼Œåªéœ€è¦æä¾›è‹±æ–‡çš„ç¿»è¯‘ç»“æœã€‚\nç°åœ¨è¯·ä½ ç¿»è¯‘ä»¥ä¸‹è¿™å¥è‹±è¯­ï¼š\n" + "The mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."

messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

- VLLM:
deploying LLMs:
```bash
python3 -m vllm.entrypoints.openai.api_server --model [model_ckpt] --served-model-name [model_name]
```

calling LLMs:
```python
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

prompt = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚ä½ åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­éå¸¸æ“…é•¿æ€è€ƒï¼Œä¼šå…ˆè¿›è¡Œæ€è€ƒå†ç»™å‡ºç¿»è¯‘ç»“æœã€‚ä½ çš„è¾“å‡ºæ ¼å¼ä¸ºï¼š\n<think>\n[æ€è€ƒè¿‡ç¨‹]\n</think>[ç¿»è¯‘ç»“æœ]\n\nåœ¨ä½ æ€è€ƒå®Œä¹‹åï¼Œä¹Ÿå°±æ˜¯</think>ä¹‹åï¼Œä½ ä¼šç»™å‡ºæœ€ç»ˆçš„ç¿»è¯‘å³â€œ[ç¿»è¯‘ç»“æœ]â€ï¼Œä¸”[ç¿»è¯‘ç»“æœ]ä¸­ä¸éœ€è¦ç»™å‡ºä»»ä½•è§£é‡Šå’Œæè¿°ï¼Œåªéœ€è¦æä¾›è‹±æ–‡çš„ç¿»è¯‘ç»“æœã€‚\nç°åœ¨è¯·ä½ ç¿»è¯‘ä»¥ä¸‹è¿™å¥è‹±è¯­ï¼š\n" + "The mother, with her feet propped up on a stool, seemed to be trying to get to the bottom of that answer, whose feminine profundity had struck her all of a heap."

chat_response = client.chat.completions.create(
    model=[model_name],
    messages=[
        {"role": "user", "content": prompt},
    ],
    temperature=0.1,
    top_p=0.8,
    max_tokens=2048,
    extra_body={
        "repetition_penalty": 1.05,
    },
)
print("Chat response:", chat_response)
```


# License
This work is licensed under cc-by-nc-sa-4.0

